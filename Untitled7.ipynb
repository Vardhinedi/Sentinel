{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vardhinedi/Sentinel/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmn6PcUdOOzu",
        "outputId": "adb12532-2dd5-4e2b-f2ba-ea27ad5f8823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated MAE: 0.1110, RÂ²: -0.0143, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Warning: Model performance below threshold (MAE < 0.1, RÂ² > 0.8)\n",
            "Test MAE: 0.1782, RÂ²: -1.7102, Precision: 0.1875, Recall: 0.2885, F1: 0.2273\n",
            "Prediction range: 0.1392 to 0.5524, mean: 0.2492\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate sequential data with correlations and anomalies\n",
        "num_samples = 1000\n",
        "timesteps = 10\n",
        "time = np.linspace(0, 1, timesteps)\n",
        "weather_data = {}\n",
        "for i in range(num_samples):\n",
        "    base_temp = np.random.uniform(-10, 40)\n",
        "    base_wind = np.random.uniform(0, 50)\n",
        "    base_pressure = np.random.uniform(950, 1050)\n",
        "    base_humidity = np.random.uniform(10, 100)\n",
        "    base_visibility = np.random.uniform(1, 20)\n",
        "    base_clouds = np.random.uniform(0, 100)\n",
        "    base_thrust = np.random.uniform(500, 1000)\n",
        "    base_pump = np.random.uniform(50, 150)\n",
        "    base_avionics = np.random.uniform(0.9, 1.0)\n",
        "    base_sensors = np.random.uniform(0.85, 1.0)\n",
        "\n",
        "    # Introduce correlations (e.g., high humidity reduces visibility)\n",
        "    humidity = base_humidity + 5 * np.sin(2 * np.pi * time) + np.random.normal(0, 0.5, timesteps)\n",
        "    visibility = base_visibility - 0.1 * humidity + 2 * np.sin(2 * np.pi * time + 2) + np.random.normal(0, 0.2, timesteps)\n",
        "    # Simulate rare wind gust anomaly\n",
        "    wind = base_wind + 3 * np.sin(2 * np.pi * time + 0.5) + np.random.normal(0, 0.5, timesteps)\n",
        "    if np.random.random() < 0.05:  # 5% chance of anomaly\n",
        "        wind[np.random.randint(0, timesteps)] += np.random.uniform(20, 30)\n",
        "\n",
        "    weather_data[f\"sample_{i}\"] = {\n",
        "        \"Temperature_C\": base_temp + 5 * np.sin(2 * np.pi * time) + np.random.normal(0, 0.5, timesteps),\n",
        "        \"Wind_Speed_kmph\": wind,\n",
        "        \"Atmospheric_Pressure_hPa\": base_pressure + 10 * np.sin(2 * np.pi * time + 1) + np.random.normal(0, 1, timesteps),\n",
        "        \"Humidity_percent\": humidity,\n",
        "        \"Visibility_km\": visibility,\n",
        "        \"Cloud_Cover_percent\": base_clouds + 5 * np.sin(2 * np.pi * time + 2.5) + np.random.normal(0, 0.5, timesteps),\n",
        "        \"Engine_Thrust_kN\": base_thrust + 10 * np.sin(2 * np.pi * time + 3) + np.random.normal(0, 1, timesteps),\n",
        "        \"Fuel_Pump_Pressure_bar\": base_pump + 5 * np.sin(2 * np.pi * time + 3.5) + np.random.normal(0, 0.5, timesteps),\n",
        "        \"Avionics_Status\": base_avionics + 0.01 * np.sin(2 * np.pi * time + 4) + np.random.normal(0, 0.005, timesteps),\n",
        "        \"Sensor_Reliability\": base_sensors + 0.01 * np.sin(2 * np.pi * time + 4.5) + np.random.normal(0, 0.005, timesteps),\n",
        "    }\n",
        "\n",
        "# Convert to DataFrame\n",
        "data = []\n",
        "for i in range(num_samples):\n",
        "    sample = weather_data[f\"sample_{i}\"]\n",
        "    sample[\"Temp_Wind_Interaction\"] = sample[\"Temperature_C\"] * sample[\"Wind_Speed_kmph\"]\n",
        "    weather_score = (\n",
        "        0.4 * (sample[\"Temperature_C\"] / 40) -\n",
        "        0.6 * (sample[\"Wind_Speed_kmph\"] / 50)**2 +\n",
        "        0.3 * (sample[\"Atmospheric_Pressure_hPa\"] - 950) / 100 -\n",
        "        0.5 * (sample[\"Cloud_Cover_percent\"] / 100)**2 -\n",
        "        0.4 * (sample[\"Humidity_percent\"] / 100)\n",
        "    )\n",
        "    technical_score = (\n",
        "        0.4 * (sample[\"Engine_Thrust_kN\"] - 500) / 500 +\n",
        "        0.3 * (sample[\"Fuel_Pump_Pressure_bar\"] - 50) / 100 +\n",
        "        0.2 * (sample[\"Avionics_Status\"] - 0.9) / 0.1 +\n",
        "        0.1 * (sample[\"Sensor_Reliability\"] - 0.85) / 0.15\n",
        "    )\n",
        "    sample[\"Launch_Feasibility_Score\"] = np.clip(0.5 * weather_score + 0.5 * technical_score, 0, 1).mean()\n",
        "    data.append(pd.DataFrame(sample))\n",
        "\n",
        "# Combine samples\n",
        "df = pd.concat(data, ignore_index=True)\n",
        "X = df.drop(columns=[\"Launch_Feasibility_Score\"])\n",
        "y = df.groupby(df.index // timesteps)[\"Launch_Feasibility_Score\"].mean()\n",
        "\n",
        "# Reshape X for LSTM\n",
        "X_array = np.array([X.iloc[i * timesteps:(i + 1) * timesteps].values for i in range(num_samples)])\n",
        "\n",
        "# Split into 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_array, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = np.array([scaler.fit_transform(sample) for sample in X_train])\n",
        "X_test_scaled = np.array([scaler.transform(sample) for sample in X_test])\n",
        "\n",
        "# Define LSTM model with Monte Carlo dropout\n",
        "features = X_train_scaled.shape[2]\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='tanh', return_sequences=True, input_shape=(timesteps, features)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, activation='tanh', return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(16, activation='tanh'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Cross-validation with additional metrics\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "mae_scores, r2_scores, precision_scores, recall_scores, f1_scores = [], [], [], [], []\n",
        "for train_idx, val_idx in kf.split(X_train_scaled):\n",
        "    X_kf_train, X_kf_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "    y_kf_train, y_kf_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "    model.fit(\n",
        "        X_kf_train, y_kf_train,\n",
        "        validation_data=(X_kf_val, y_kf_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=10), ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)],\n",
        "        verbose=0\n",
        "    )\n",
        "    y_kf_pred = model.predict(X_kf_val, verbose=0)\n",
        "    mae_scores.append(mean_absolute_error(y_kf_val, y_kf_pred))\n",
        "    r2_scores.append(r2_score(y_kf_val, y_kf_pred))\n",
        "    y_kf_pred_binary = (y_kf_pred >= np.percentile(y_kf_train, 75)).astype(int)\n",
        "    y_kf_val_binary = (y_kf_val >= np.percentile(y_kf_train, 75)).astype(int)\n",
        "    precision_scores.append(precision_score(y_kf_val_binary, y_kf_pred_binary))\n",
        "    recall_scores.append(recall_score(y_kf_val_binary, y_kf_pred_binary))\n",
        "    f1_scores.append(f1_score(y_kf_val_binary, y_kf_pred_binary))\n",
        "\n",
        "# Evaluate model performance\n",
        "mean_mae, mean_r2 = np.mean(mae_scores), np.mean(r2_scores)\n",
        "mean_precision, mean_recall, mean_f1 = np.mean(precision_scores), np.mean(recall_scores), np.mean(f1_scores)\n",
        "print(f\"Cross-validated MAE: {mean_mae:.4f}, RÂ²: {mean_r2:.4f}, Precision: {mean_precision:.4f}, Recall: {mean_recall:.4f}, F1: {mean_f1:.4f}\")\n",
        "if mean_mae > 0.1 or mean_r2 < 0.8:\n",
        "    print(\"Warning: Model performance below threshold (MAE < 0.1, RÂ² > 0.8)\")\n",
        "\n",
        "# Train final model\n",
        "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, callbacks=[EarlyStopping(monitor='loss', patience=10)], verbose=0)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = model.predict(X_test_scaled, verbose=0)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "y_pred_binary = (y_pred >= np.percentile(y_train, 75)).astype(int)\n",
        "y_test_binary = (y_test >= np.percentile(y_train, 75)).astype(int)\n",
        "precision = precision_score(y_test_binary, y_pred_binary)\n",
        "recall = recall_score(y_test_binary, y_pred_binary)\n",
        "f1 = f1_score(y_test_binary, y_pred_binary)\n",
        "print(f\"Test MAE: {mae:.4f}, RÂ²: {r2:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "print(f\"Prediction range: {y_pred.min():.4f} to {y_pred.max():.4f}, mean: {y_pred.mean():.4f}\")\n",
        "\n",
        "# Dynamic threshold (stricter for manned launches)\n",
        "is_manned = True  # Example context\n",
        "dynamic_threshold = np.percentile(y_train, 80 if is_manned else 75)\n",
        "\n",
        "# Input validation ranges\n",
        "ranges = {\n",
        "    \"Temperature_C\": (-10, 40),\n",
        "    \"Wind_Speed_kmph\": (0, 50),\n",
        "    \"Atmospheric_Pressure_hPa\": (950, 1050),\n",
        "    \"Humidity_percent\": (10, 100),\n",
        "    \"Visibility_km\": (1, 20),\n",
        "    \"Cloud_Cover_percent\": (0, 100),\n",
        "    \"Engine_Thrust_kN\": (500, 1000),\n",
        "    \"Fuel_Pump_Pressure_bar\": (50, 150),\n",
        "    \"Avionics_Status\": (0.9, 1.0),\n",
        "    \"Sensor_Reliability\": (0.85, 1.0)\n",
        "}\n",
        "\n",
        "# Define thresholds for alerts\n",
        "thresholds = {\n",
        "    \"Temperature_C\": {\"min\": 0, \"max\": 35},\n",
        "    \"Wind_Speed_kmph\": {\"max\": 30},\n",
        "    \"Atmospheric_Pressure_hPa\": {\"min\": 970, \"max\": 1030},\n",
        "    \"Humidity_percent\": {\"max\": 85},\n",
        "    \"Visibility_km\": {\"min\": 5},\n",
        "    \"Cloud_Cover_percent\": {\"max\": 70},\n",
        "    \"Engine_Thrust_kN\": {\"min\": 600},\n",
        "    \"Fuel_Pump_Pressure_bar\": {\"min\": 70},\n",
        "    \"Avionics_Status\": {\"min\": 0.95},\n",
        "    \"Sensor_Reliability\": {\"min\": 0.90}\n",
        "}\n",
        "\n",
        "# Prediction function with input validation and visualization\n",
        "def predict_launch_feasibility(temp, wind, pressure, humidity, visibility, clouds, thrust, pump_pressure, avionics, sensors, csv_file=None):\n",
        "    # Handle CSV batch prediction\n",
        "    if csv_file is not None:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file)\n",
        "            required_columns = list(ranges.keys())\n",
        "            if not all(col in df.columns for col in required_columns):\n",
        "                return f\"Error: CSV must contain columns: {', '.join(required_columns)}\"\n",
        "            results = []\n",
        "            for _, row in df.iterrows():\n",
        "                result = process_single_prediction(row['Temperature_C'], row['Wind_Speed_kmph'], row['Atmospheric_Pressure_hPa'],\n",
        "                                                  row['Humidity_percent'], row['Visibility_km'], row['Cloud_Cover_percent'],\n",
        "                                                  row['Engine_Thrust_kN'], row['Fuel_Pump_Pressure_bar'], row['Avionics_Status'],\n",
        "                                                  row['Sensor_Reliability'])\n",
        "                results.append(result)\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            return f\"CSV processing error: {str(e)}\"\n",
        "\n",
        "    return process_single_prediction(temp, wind, pressure, humidity, visibility, clouds, thrust, pump_pressure, avionics, sensors)\n",
        "\n",
        "def process_single_prediction(temp, wind, pressure, humidity, visibility, clouds, thrust, pump_pressure, avionics, sensors):\n",
        "    # Input validation\n",
        "    inputs = {\n",
        "        \"Temperature_C\": temp, \"Wind_Speed_kmph\": wind, \"Atmospheric_Pressure_hPa\": pressure,\n",
        "        \"Humidity_percent\": humidity, \"Visibility_km\": visibility, \"Cloud_Cover_percent\": clouds,\n",
        "        \"Engine_Thrust_kN\": thrust, \"Fuel_Pump_Pressure_bar\": pump_pressure,\n",
        "        \"Avionics_Status\": avionics, \"Sensor_Reliability\": sensors\n",
        "    }\n",
        "    for feature, value in inputs.items():\n",
        "        min_val, max_val = ranges.get(feature, (-float('inf'), float('inf')))\n",
        "        if not (min_val <= value <= max_val):\n",
        "            return f\"Error: {feature} value {value} out of valid range [{min_val}, {max_val}]\"\n",
        "\n",
        "    # Create sequential input\n",
        "    input_data = np.zeros((timesteps, 11))\n",
        "    time = np.linspace(0, 1, timesteps)\n",
        "    for i, t in enumerate(time):\n",
        "        input_data[i] = [\n",
        "            temp + 5 * np.sin(2 * np.pi * t),\n",
        "            wind + 3 * np.sin(2 * np.pi * t + 0.5),\n",
        "            pressure + 10 * np.sin(2 * np.pi * t + 1),\n",
        "            humidity + 5 * np.sin(2 * np.pi * t + 1.5),\n",
        "            visibility + 2 * np.sin(2 * np.pi * t + 2),\n",
        "            clouds + 5 * np.sin(2 * np.pi * t + 2.5),\n",
        "            thrust + 10 * np.sin(2 * np.pi * t + 3),\n",
        "            pump_pressure + 5 * np.sin(2 * np.pi * t + 3.5),\n",
        "            avionics + 0.01 * np.sin(2 * np.pi * t + 4),\n",
        "            sensors + 0.01 * np.sin(2 * np.pi * t + 4.5),\n",
        "            (temp + 5 * np.sin(2 * np.pi * t)) * (wind + 3 * np.sin(2 * np.pi * t + 0.5))\n",
        "        ]\n",
        "\n",
        "    # Monte Carlo dropout for uncertainty\n",
        "    try:\n",
        "        input_scaled = scaler.transform(input_data)\n",
        "        predictions = []\n",
        "        for _ in range(10):  # 10 Monte Carlo samples\n",
        "            predictions.append(model.predict(input_scaled.reshape(1, timesteps, features), verbose=0)[0][0])\n",
        "        score = np.mean(predictions)\n",
        "        uncertainty = np.std(predictions)\n",
        "    except Exception as e:\n",
        "        return f\"Prediction error: {str(e)}\"\n",
        "\n",
        "    # Check thresholds using max/min values\n",
        "    max_inputs = {k: np.max([input_data[i][list(inputs.keys()).index(k)] for i in range(timesteps)]) for k in inputs}\n",
        "    min_inputs = {k: np.min([input_data[i][list(inputs.keys()).index(k)] for i in range(timesteps)]) for k in inputs}\n",
        "    violations = []\n",
        "    for feature in inputs:\n",
        "        thresh = thresholds.get(feature, {})\n",
        "        if \"min\" in thresh and min_inputs[feature] < thresh[\"min\"]:\n",
        "            violations.append(f\"{feature}: {min_inputs[feature]:.2f} < {thresh['min']}\")\n",
        "        if \"max\" in thresh and max_inputs[feature] > thresh[\"max\"]:\n",
        "            violations.append(f\"{feature}: {max_inputs[feature]:.2f} > {thresh['max']}\")\n",
        "\n",
        "    # Decision\n",
        "    decision = \"Good to go\" if score >= dynamic_threshold else \"No go\"\n",
        "    alert = \"ðŸš¨ RED ALERT ðŸš¨\\n\" + \"\\n\".join(violations) if decision == \"No go\" or violations else \"No alert\"\n",
        "\n",
        "    # Plot temporal trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(time, input_data[:, 0], label=\"Temperature (Â°C)\")\n",
        "    plt.plot(time, input_data[:, 1], label=\"Wind Speed (kmph)\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.title(\"Temporal Trends of Key Inputs\")\n",
        "    plt.legend()\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "    img_str = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return (f\"Launch Feasibility Score: {score:.4f} Â± {uncertainty:.4f}\",\n",
        "            f\"Decision: {decision} (Threshold: {dynamic_threshold:.4f})\",\n",
        "            f\"Threshold Violations: {', '.join(violations) if violations else 'None'}\",\n",
        "            alert,\n",
        "            f'<img src=\"data:image/png;base64,{img_str}\"/>')\n",
        "\n",
        "# Gradio interface\n",
        "launch_predictor_interface = gr.Interface(\n",
        "    fn=predict_launch_feasibility,\n",
        "    inputs=[\n",
        "        gr.Slider(-10, 40, label=\"Temperature (Â°C)\", value=25),\n",
        "        gr.Slider(0, 50, label=\"Wind Speed (kmph)\", value=10),\n",
        "        gr.Slider(950, 1050, label=\"Atmospheric Pressure (hPa)\", value=1010),\n",
        "        gr.Slider(10, 100, label=\"Humidity (%)\", value=50),\n",
        "        gr.Slider(1, 20, label=\"Visibility (km)\", value=15),\n",
        "        gr.Slider(0, 100, label=\"Cloud Cover (%)\", value=20),\n",
        "        gr.Slider(500, 1000, label=\"Engine Thrust (kN)\", value=750),\n",
        "        gr.Slider(50, 150, label=\"Fuel Pump Pressure (bar)\", value=100),\n",
        "        gr.Slider(0.9, 1.0, label=\"Avionics Status (0.9-1.0)\", value=0.95),\n",
        "        gr.Slider(0.85, 1.0, label=\"Sensor Reliability (0.85-1.0)\", value=0.95),\n",
        "        gr.File(label=\"Upload CSV for batch prediction\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Feasibility Score\"),\n",
        "        gr.Textbox(label=\"Launch Decision\"),\n",
        "        gr.Textbox(label=\"Threshold Violations\"),\n",
        "        gr.Textbox(label=\"Alert\"),\n",
        "        gr.HTML(label=\"Temporal Trends\")\n",
        "    ],\n",
        "    title=\"Rocket Launch Feasibility Predictor\",\n",
        "    description=\"Enter weather and technical data to predict launch feasibility. Upload a CSV for batch predictions. Alerts show for 'No go' or threshold violations.\"\n",
        ")\n",
        "\n",
        "# Note: For deployment, consider model quantization using TensorFlow Lite\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLYhd2ZEKKnz",
        "outputId": "e3184c0d-be66-4f5c-8270-c01ca3cadaaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated MAE: 0.0604, RÂ²: 0.7718, Precision: 0.8592, Recall: 0.8421, F1: 0.8500\n",
            "Warning: Model performance below threshold (MAE < 0.1, RÂ² > 0.8)\n",
            "Test MAE: 0.3775, RÂ²: -6.0817, Precision: 0.3667, Recall: 0.2418, F1: 0.2914\n",
            "Prediction range: 0.1313 to 0.9764, mean: 0.4121\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate sequential data with correlations and anomalies\n",
        "num_samples = 1000\n",
        "timesteps = 10\n",
        "time = np.linspace(0, 1, timesteps)\n",
        "data = {}\n",
        "for i in range(num_samples):\n",
        "    base_hours = np.random.uniform(10, 500)\n",
        "    base_vibration = np.random.uniform(0.1, 5.0)\n",
        "    base_temp = np.random.uniform(50, 200)\n",
        "    base_pressure = np.random.uniform(20, 100)\n",
        "    base_flow = np.random.uniform(5, 50)\n",
        "\n",
        "    # Introduce correlations (e.g., high vibration increases temperature)\n",
        "    vibration = base_vibration + 0.5 * np.sin(2 * np.pi * time + 0.5) + np.random.normal(0, 0.1, timesteps)\n",
        "    temp = base_temp + 0.2 * vibration + 5 * np.sin(2 * np.pi * time + 1) + np.random.normal(0, 0.5, timesteps)\n",
        "    # Simulate rare vibration spike\n",
        "    if np.random.random() < 0.05:  # 5% chance of anomaly\n",
        "        vibration[np.random.randint(0, timesteps)] += np.random.uniform(2, 3)\n",
        "\n",
        "    data[f\"sample_{i}\"] = {\n",
        "        \"Operating_Hours\": base_hours + 10 * np.sin(2 * np.pi * time) + np.random.normal(0, 1, timesteps),\n",
        "        \"Vibration_Level_g\": vibration,\n",
        "        \"Temperature_degC\": temp,\n",
        "        \"Pressure_bar\": base_pressure + 2 * np.sin(2 * np.pi * time + 1.5) + np.random.normal(0, 0.2, timesteps),\n",
        "        \"Fuel_Flow_Rate_lps\": base_flow + 2 * np.sin(2 * np.pi * time + 2) + np.random.normal(0, 0.2, timesteps),\n",
        "    }\n",
        "\n",
        "# Convert to DataFrame\n",
        "data_frames = []\n",
        "for i in range(num_samples):\n",
        "    sample = data[f\"sample_{i}\"]\n",
        "    sample[\"Vibration_Temp_Interaction\"] = sample[\"Vibration_Level_g\"] * sample[\"Temperature_degC\"]\n",
        "    score = (\n",
        "        0.35 * (sample[\"Operating_Hours\"] / 500) +  # Increased weight\n",
        "        0.3 * (sample[\"Vibration_Level_g\"] / 5.0) +  # Increased weight\n",
        "        0.25 * (sample[\"Temperature_degC\"] - 50) / 150 +\n",
        "        0.15 * (sample[\"Pressure_bar\"] - 20) / 80 +\n",
        "        0.15 * (sample[\"Fuel_Flow_Rate_lps\"] - 5) / 45  # Increased weight\n",
        "    )\n",
        "    sample[\"Maintenance_Need_Score\"] = np.clip(score, 0, 1).mean()\n",
        "    data_frames.append(pd.DataFrame(sample))\n",
        "\n",
        "# Combine samples\n",
        "df = pd.concat(data_frames, ignore_index=True)\n",
        "X = df.drop(columns=[\"Maintenance_Need_Score\"])\n",
        "y = df.groupby(df.index // timesteps)[\"Maintenance_Need_Score\"].mean()\n",
        "\n",
        "# Reshape X for LSTM with time-decay weighting\n",
        "X_array = np.array([X.iloc[i * timesteps:(i + 1) * timesteps].values for i in range(num_samples)])\n",
        "for i in range(num_samples):\n",
        "    decay = np.linspace(1, 0.7, timesteps)\n",
        "    X_array[i] = X_array[i] * decay[:, np.newaxis]\n",
        "\n",
        "# Split into 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_array, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = np.array([scaler.fit_transform(sample) for sample in X_train])\n",
        "X_test_scaled = np.array([scaler.transform(sample) for sample in X_test])\n",
        "\n",
        "# Define LSTM model with Monte Carlo dropout\n",
        "features = X_train_scaled.shape[2]\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='tanh', return_sequences=True, input_shape=(timesteps, features)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, activation='tanh', return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(16, activation='tanh'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Cross-validation with additional metrics\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "mae_scores, r2_scores, precision_scores, recall_scores, f1_scores = [], [], [], [], []\n",
        "for train_idx, val_idx in kf.split(X_train_scaled):\n",
        "    X_kf_train, X_kf_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "    y_kf_train, y_kf_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "    model.fit(\n",
        "        X_kf_train, y_kf_train,\n",
        "        validation_data=(X_kf_val, y_kf_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=10), ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)],\n",
        "        verbose=0\n",
        "    )\n",
        "    y_kf_pred = model.predict(X_kf_val, verbose=0)\n",
        "    mae_scores.append(mean_absolute_error(y_kf_val, y_kf_pred))\n",
        "    r2_scores.append(r2_score(y_kf_val, y_kf_pred))\n",
        "    y_kf_pred_binary = (y_kf_pred >= np.percentile(y_kf_train, 50)).astype(int)\n",
        "    y_kf_val_binary = (y_kf_val >= np.percentile(y_kf_train, 50)).astype(int)\n",
        "    precision_scores.append(precision_score(y_kf_val_binary, y_kf_pred_binary))\n",
        "    recall_scores.append(recall_score(y_kf_val_binary, y_kf_pred_binary))\n",
        "    f1_scores.append(f1_score(y_kf_val_binary, y_kf_pred_binary))\n",
        "\n",
        "# Evaluate model performance\n",
        "mean_mae, mean_r2 = np.mean(mae_scores), np.mean(r2_scores)\n",
        "mean_precision, mean_recall, mean_f1 = np.mean(precision_scores), np.mean(recall_scores), np.mean(f1_scores)\n",
        "print(f\"Cross-validated MAE: {mean_mae:.4f}, RÂ²: {mean_r2:.4f}, Precision: {mean_precision:.4f}, Recall: {mean_recall:.4f}, F1: {mean_f1:.4f}\")\n",
        "if mean_mae > 0.1 or mean_r2 < 0.8:\n",
        "    print(\"Warning: Model performance below threshold (MAE < 0.1, RÂ² > 0.8)\")\n",
        "\n",
        "# Train final model\n",
        "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, callbacks=[EarlyStopping(monitor='loss', patience=10)], verbose=0)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = model.predict(X_test_scaled, verbose=0)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "y_pred_binary = (y_pred >= np.percentile(y_train, 50)).astype(int)\n",
        "y_test_binary = (y_test >= np.percentile(y_train, 50)).astype(int)\n",
        "precision = precision_score(y_test_binary, y_pred_binary)\n",
        "recall = recall_score(y_test_binary, y_pred_binary)\n",
        "f1 = f1_score(y_test_binary, y_pred_binary)\n",
        "print(f\"Test MAE: {mae:.4f}, RÂ²: {r2:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "print(f\"Prediction range: {y_pred.min():.4f} to {y_pred.max():.4f}, mean: {y_pred.mean():.4f}\")\n",
        "\n",
        "# Dynamic threshold\n",
        "dynamic_threshold = np.percentile(y_train, 50)\n",
        "\n",
        "# Input validation ranges\n",
        "ranges = {\n",
        "    \"Operating_Hours\": (10, 500),\n",
        "    \"Vibration_Level_g\": (0.1, 5.0),\n",
        "    \"Temperature_degC\": (50, 200),\n",
        "    \"Pressure_bar\": (20, 100),\n",
        "    \"Fuel_Flow_Rate_lps\": (5, 50)\n",
        "}\n",
        "\n",
        "# Define thresholds for alerts\n",
        "thresholds = {\n",
        "    \"Operating_Hours\": {\"max\": 400},\n",
        "    \"Vibration_Level_g\": {\"max\": 3.0},\n",
        "    \"Temperature_degC\": {\"max\": 180},\n",
        "    \"Pressure_bar\": {\"max\": 90},\n",
        "    \"Fuel_Flow_Rate_lps\": {\"min\": 10}\n",
        "}\n",
        "\n",
        "# Prediction function with input validation and visualization\n",
        "def predict_maintenance_need(hours, vibration, temp, pressure, flow, csv_file=None):\n",
        "    # Handle CSV batch prediction\n",
        "    if csv_file is not None:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file)\n",
        "            required_columns = list(ranges.keys())\n",
        "            if not all(col in df.columns for col in required_columns):\n",
        "                return f\"Error: CSV must contain columns: {', '.join(required_columns)}\"\n",
        "            results = []\n",
        "            for _, row in df.iterrows():\n",
        "                result = process_single_prediction(row['Operating_Hours'], row['Vibration_Level_g'], row['Temperature_degC'],\n",
        "                                                  row['Pressure_bar'], row['Fuel_Flow_Rate_lps'])\n",
        "                results.append(result)\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            return f\"CSV processing error: {str(e)}\"\n",
        "\n",
        "    return process_single_prediction(hours, vibration, temp, pressure, flow)\n",
        "\n",
        "def process_single_prediction(hours, vibration, temp, pressure, flow):\n",
        "    # Input validation\n",
        "    inputs = {\n",
        "        \"Operating_Hours\": hours, \"Vibration_Level_g\": vibration,\n",
        "        \"Temperature_degC\": temp, \"Pressure_bar\": pressure,\n",
        "        \"Fuel_Flow_Rate_lps\": flow\n",
        "    }\n",
        "    for feature, value in inputs.items():\n",
        "        min_val, max_val = ranges.get(feature, (-float('inf'), float('inf')))\n",
        "        if not (min_val <= value <= max_val):\n",
        "            return f\"Error: {feature} value {value} out of valid range [{min_val}, {max_val}]\"\n",
        "\n",
        "    # Create sequential input with time-decay\n",
        "    input_data = np.zeros((timesteps, 6))\n",
        "    time = np.linspace(0, 1, timesteps)\n",
        "    decay = np.linspace(1, 0.7, timesteps)\n",
        "    for i, t in enumerate(time):\n",
        "        input_data[i] = [\n",
        "            hours + 10 * np.sin(2 * np.pi * t),\n",
        "            vibration + 0.5 * np.sin(2 * np.pi * t + 0.5),\n",
        "            temp + 5 * np.sin(2 * np.pi * t + 1),\n",
        "            pressure + 2 * np.sin(2 * np.pi * t + 1.5),\n",
        "            flow + 2 * np.sin(2 * np.pi * t + 2),\n",
        "            (vibration + 0.5 * np.sin(2 * np.pi * t + 0.5)) * (temp + 5 * np.sin(2 * np.pi * t + 1))\n",
        "        ]\n",
        "    input_data *= decay[:, np.newaxis]\n",
        "\n",
        "    # Monte Carlo dropout for uncertainty\n",
        "    try:\n",
        "        input_scaled = scaler.transform(input_data)\n",
        "        predictions = []\n",
        "        for _ in range(10):  # 10 Monte Carlo samples\n",
        "            predictions.append(model.predict(input_scaled.reshape(1, timesteps, features), verbose=0)[0][0])\n",
        "        score = np.mean(predictions)\n",
        "        uncertainty = np.std(predictions)\n",
        "    except Exception as e:\n",
        "        return f\"Prediction error: {str(e)}\"\n",
        "\n",
        "    # Check thresholds using max values\n",
        "    max_inputs = {k: np.max([input_data[i][list(inputs.keys()).index(k)] for i in range(timesteps)]) for k in inputs}\n",
        "    violations = []\n",
        "    for feature, value in max_inputs.items():\n",
        "        thresh = thresholds.get(feature, {})\n",
        "        if \"min\" in thresh and value < thresh[\"min\"]:\n",
        "            violations.append(f\"{feature}: {value:.2f} < {thresh['min']}\")\n",
        "        if \"max\" in thresh and value > thresh[\"max\"]:\n",
        "            violations.append(f\"{feature}: {value:.2f} > {thresh['max']}\")\n",
        "\n",
        "    # Decision with threshold override\n",
        "    has_violations = len(violations) > 0\n",
        "    decision = \"Maintenance Urgent\" if score >= dynamic_threshold or has_violations else \"No Maintenance Needed\"\n",
        "    alert = \"ðŸš¨ RED ALERT ðŸš¨\\n\" + \"\\n\".join(violations) if decision == \"Maintenance Urgent\" or has_violations else \"No alert\"\n",
        "\n",
        "    # Plot temporal trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(time, input_data[:, 1], label=\"Vibration Level (g)\")\n",
        "    plt.plot(time, input_data[:, 2], label=\"Temperature (Â°C)\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.title(\"Temporal Trends of Key Inputs\")\n",
        "    plt.legend()\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "    img_str = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return (f\"Maintenance Need Score: {score:.4f} Â± {uncertainty:.4f}\",\n",
        "            f\"Decision: {decision} (Threshold: {dynamic_threshold:.4f})\",\n",
        "            f\"Threshold Violations: {', '.join(violations) if violations else 'None'}\",\n",
        "            alert,\n",
        "            f'<img src=\"data:image/png;base64,{img_str}\"/>')\n",
        "\n",
        "# Gradio interface\n",
        "maintenance_predictor_interface = gr.Interface(\n",
        "    fn=predict_maintenance_need,\n",
        "    inputs=[\n",
        "        gr.Slider(10, 500, label=\"Operating Hours\", value=100),\n",
        "        gr.Slider(0.1, 5.0, label=\"Vibration Level (g)\", value=1.0),\n",
        "        gr.Slider(50, 200, label=\"Temperature (Â°C)\", value=100),\n",
        "        gr.Slider(20, 100, label=\"Pressure (bar)\", value=50),\n",
        "        gr.Slider(5, 50, label=\"Fuel Flow Rate (l/s)\", value=25),\n",
        "        gr.File(label=\"Upload CSV for batch prediction\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Maintenance Score\"),\n",
        "        gr.Textbox(label=\"Maintenance Decision\"),\n",
        "        gr.Textbox(label=\"Threshold Violations\"),\n",
        "        gr.Textbox(label=\"Alert\"),\n",
        "        gr.HTML(label=\"Temporal Trends\")\n",
        "    ],\n",
        "    title=\"Rocket Engine Maintenance Predictor\",\n",
        "    description=\"Enter engine usage and sensor data to predict maintenance needs. Upload a CSV for batch predictions. Alerts show for urgent cases or violations.\"\n",
        ")\n",
        "\n",
        "# Note: For deployment, consider model quantization using TensorFlow Lite\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjsNdUqeKPh3",
        "outputId": "6b50d212-81da-4c73-bc5a-148c0841d437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated MAE: 0.1555, RÂ²: -0.0059\n",
            "Warning: Model performance below threshold (MAE < 0.1, RÂ² > 0.8)\n",
            "Test MAE: 0.1484, RÂ²: -0.0613\n",
            "Prediction range: 0.3823 to 0.5441, mean: 0.5023\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import gradio as gr\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate sequential data (1000 samples, 10 timesteps)\n",
        "num_samples = 1000\n",
        "timesteps = 10\n",
        "time = np.linspace(0, 1, timesteps)\n",
        "data = {}\n",
        "for i in range(num_samples):\n",
        "    base_altitude = np.random.uniform(400, 800)\n",
        "    base_inclination = np.random.uniform(0, 90)\n",
        "    base_ground = np.random.uniform(0.7, 1.0)\n",
        "    base_power = np.random.uniform(60, 100)\n",
        "    base_storage = np.random.uniform(20, 90)\n",
        "    base_priority = np.random.uniform(0.5, 1.0)\n",
        "\n",
        "    data[f\"sample_{i}\"] = {\n",
        "        \"Orbit_Altitude_km\": base_altitude + 10 * np.sin(2 * np.pi * time) + np.random.normal(0, 1, timesteps),\n",
        "        \"Inclination_deg\": base_inclination + 5 * np.sin(2 * np.pi * time + 0.5) + np.random.normal(0, 0.5, timesteps),\n",
        "        \"Ground_Station_Availability\": base_ground + 0.05 * np.sin(2 * np.pi * time + 1) + np.random.normal(0, 0.01, timesteps),\n",
        "        \"Power_Level_percent\": base_power + 2 * np.sin(2 * np.pi * time + 1.5) + np.random.normal(0, 0.2, timesteps),\n",
        "        \"Data_Storage_percent\": base_storage + 5 * np.sin(2 * np.pi * time + 2) + np.random.normal(0, 0.5, timesteps),\n",
        "        \"Target_Priority\": base_priority + 0.05 * np.sin(2 * np.pi * time + 2.5) + np.random.normal(0, 0.01, timesteps),\n",
        "    }\n",
        "\n",
        "# Convert to DataFrame\n",
        "data_frames = []\n",
        "for i in range(num_samples):\n",
        "    sample = data[f\"sample_{i}\"]\n",
        "    sample[\"Altitude_Power_Interaction\"] = sample[\"Orbit_Altitude_km\"] * sample[\"Power_Level_percent\"]\n",
        "    score = (\n",
        "        0.3 * (sample[\"Orbit_Altitude_km\"] - 400) / 400 +\n",
        "        0.2 * (sample[\"Inclination_deg\"] / 90) +\n",
        "        0.3 * (sample[\"Ground_Station_Availability\"] - 0.7) / 0.3 +\n",
        "        0.2 * (sample[\"Power_Level_percent\"] - 60) / 40 -\n",
        "        0.4 * (sample[\"Data_Storage_percent\"] / 100) +\n",
        "        0.3 * (sample[\"Target_Priority\"] - 0.5) / 0.5\n",
        "    )\n",
        "    sample[\"Task_Priority_Score\"] = np.clip(score, 0, 1).mean()\n",
        "    data_frames.append(pd.DataFrame(sample))\n",
        "\n",
        "# Combine samples\n",
        "df = pd.concat(data_frames, ignore_index=True)\n",
        "X = df.drop(columns=[\"Task_Priority_Score\"])\n",
        "y = df.groupby(df.index // timesteps)[\"Task_Priority_Score\"].mean()\n",
        "\n",
        "# Reshape X for LSTM\n",
        "X_array = np.array([X.iloc[i * timesteps:(i + 1) * timesteps].values for i in range(num_samples)])\n",
        "\n",
        "# Split into 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_array, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = np.array([scaler.fit_transform(sample) for sample in X_train])\n",
        "X_test_scaled = np.array([scaler.transform(sample) for sample in X_test])\n",
        "\n",
        "# Define LSTM model\n",
        "features = X_train_scaled.shape[2]\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='tanh', return_sequences=True, input_shape=(timesteps, features)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, activation='tanh'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "mae_scores, r2_scores = [], []\n",
        "for train_idx, val_idx in kf.split(X_train_scaled):\n",
        "    X_kf_train, X_kf_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "    y_kf_train, y_kf_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "    model.fit(\n",
        "        X_kf_train, y_kf_train,\n",
        "        validation_data=(X_kf_val, y_kf_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=10), ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)],\n",
        "        verbose=0\n",
        "    )\n",
        "    y_kf_pred = model.predict(X_kf_val, verbose=0)\n",
        "    mae_scores.append(mean_absolute_error(y_kf_val, y_kf_pred))\n",
        "    r2_scores.append(r2_score(y_kf_val, y_kf_pred))\n",
        "\n",
        "# Evaluate model performance\n",
        "mean_mae, mean_r2 = np.mean(mae_scores), np.mean(r2_scores)\n",
        "print(f\"Cross-validated MAE: {mean_mae:.4f}, RÂ²: {mean_r2:.4f}\")\n",
        "if mean_mae > 0.1 or mean_r2 < 0.8:\n",
        "    print(\"Warning: Model performance below threshold (MAE < 0.1, RÂ² > 0.8)\")\n",
        "\n",
        "# Train final model\n",
        "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, callbacks=[EarlyStopping(monitor='loss', patience=10)], verbose=0)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = model.predict(X_test_scaled, verbose=0)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Test MAE: {mae:.4f}, RÂ²: {r2:.4f}\")\n",
        "print(f\"Prediction range: {y_pred.min():.4f} to {y_pred.max():.4f}, mean: {y_pred.mean():.4f}\")\n",
        "\n",
        "# Dynamic threshold\n",
        "dynamic_threshold = np.percentile(y_train, 75)\n",
        "\n",
        "# Input validation ranges\n",
        "ranges = {\n",
        "    \"Orbit_Altitude_km\": (400, 800),\n",
        "    \"Inclination_deg\": (0, 90),\n",
        "    \"Ground_Station_Availability\": (0.7, 1.0),\n",
        "    \"Power_Level_percent\": (60, 100),\n",
        "    \"Data_Storage_percent\": (20, 90),\n",
        "    \"Target_Priority\": (0.5, 1.0)\n",
        "}\n",
        "\n",
        "# Define thresholds for alerts\n",
        "thresholds = {\n",
        "    \"Orbit_Altitude_km\": {\"min\": 450},\n",
        "    \"Inclination_deg\": {\"min\": 30},\n",
        "    \"Ground_Station_Availability\": {\"min\": 0.8},\n",
        "    \"Power_Level_percent\": {\"min\": 70},\n",
        "    \"Data_Storage_percent\": {\"max\": 80},\n",
        "    \"Target_Priority\": {\"min\": 0.6}\n",
        "}\n",
        "\n",
        "# Prediction function with input validation\n",
        "def predict_task_priority(altitude, inclination, ground_station, power, storage, priority):\n",
        "    # Input validation\n",
        "    inputs = {\n",
        "        \"Orbit_Altitude_km\": altitude, \"Inclination_deg\": inclination,\n",
        "        \"Ground_Station_Availability\": ground_station, \"Power_Level_percent\": power,\n",
        "        \"Data_Storage_percent\": storage, \"Target_Priority\": priority\n",
        "    }\n",
        "    for feature, value in inputs.items():\n",
        "        min_val, max_val = ranges.get(feature, (-float('inf'), float('inf')))\n",
        "        if not (min_val <= value <= max_val):\n",
        "            return f\"Error: {feature} value {value} out of valid range [{min_val}, {max_val}]\"\n",
        "\n",
        "    # Create sequential input\n",
        "    input_data = np.zeros((timesteps, 7))\n",
        "    time = np.linspace(0, 1, timesteps)\n",
        "    for i, t in enumerate(time):\n",
        "        input_data[i] = [\n",
        "            altitude + 10 * np.sin(2 * np.pi * t),\n",
        "            inclination + 5 * np.sin(2 * np.pi * t + 0.5),\n",
        "            ground_station + 0.05 * np.sin(2 * np.pi * t + 1),\n",
        "            power + 2 * np.sin(2 * np.pi * t + 1.5),\n",
        "            storage + 5 * np.sin(2 * np.pi * t + 2),\n",
        "            priority + 0.05 * np.sin(2 * np.pi * t + 2.5),\n",
        "            (altitude + 10 * np.sin(2 * np.pi * t)) * (power + 2 * np.sin(2 * np.pi * t + 1.5))\n",
        "        ]\n",
        "    try:\n",
        "        input_scaled = scaler.transform(input_data)\n",
        "        score = model.predict(input_scaled.reshape(1, timesteps, features), verbose=0)[0][0]\n",
        "    except Exception as e:\n",
        "        return f\"Prediction error: {str(e)}\"\n",
        "\n",
        "    # Check thresholds (using mean values)\n",
        "    mean_inputs = {k: np.mean([input_data[i][list(inputs.keys()).index(k)] for i in range(timesteps)]) for k in inputs}\n",
        "    violations = []\n",
        "    for feature, value in mean_inputs.items():\n",
        "        thresh = thresholds.get(feature, {})\n",
        "        if \"min\" in thresh and value < thresh[\"min\"]:\n",
        "            violations.append(f\"{feature}: {value:.2f} < {thresh['min']}\")\n",
        "        if \"max\" in thresh and value > thresh[\"max\"]:\n",
        "            violations.append(f\"{feature}: {value:.2f} > {thresh['max']}\")\n",
        "\n",
        "    # Decision\n",
        "    decision = \"High Priority\" if score >= dynamic_threshold else \"Low Priority\"\n",
        "    alert = \"ðŸš¨ RED ALERT ðŸš¨\\n\" + \"\\n\".join(violations) if decision == \"Low Priority\" or violations else \"No alert\"\n",
        "\n",
        "    return (f\"Task Priority Score: {score:.4f}\",\n",
        "            f\"Decision: {decision} (Threshold: {dynamic_threshold:.4f})\",\n",
        "            f\"Threshold Violations: {', '.join(violations) if violations else 'None'}\",\n",
        "            alert)\n",
        "\n",
        "# Gradio interface\n",
        "satellite_optimizer_interface = gr.Interface(\n",
        "    fn=predict_task_priority,\n",
        "    inputs=[\n",
        "        gr.Slider(400, 800, label=\"Orbit Altitude (km)\", value=600),\n",
        "        gr.Slider(0, 90, label=\"Inclination (deg)\", value=45),\n",
        "        gr.Slider(0.7, 1.0, label=\"Ground Station Availability (0.7-1.0)\", value=0.9),\n",
        "        gr.Slider(60, 100, label=\"Power Level (%)\", value=85),\n",
        "        gr.Slider(20, 90, label=\"Data Storage Used (%)\", value=50),\n",
        "        gr.Slider(0.5, 1.0, label=\"Target Priority (0.5-1.0)\", value=0.8),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Priority Score\"),\n",
        "        gr.Textbox(label=\"Task Decision\"),\n",
        "        gr.Textbox(label=\"Threshold Violations\"),\n",
        "        gr.Textbox(label=\"Alert\")\n",
        "    ],\n",
        "    title=\"Satellite Tasking Optimizer\",\n",
        "    description=\"Enter orbital and operational data to prioritize satellite tasks. Alerts show for low priority or violations.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This would be in a new cell at the very end of your notebook,\n",
        "# AFTER all the previous cells defining models and interfaces have been run.\n",
        "\n",
        "# Assuming your interfaces are stored in these variables:\n",
        "# launch_predictor_interface\n",
        "# maintenance_predictor_interface\n",
        "# satellite_optimizer_interface\n",
        "\n",
        "combined_dashboard = gr.TabbedInterface(\n",
        "    [launch_predictor_interface, maintenance_predictor_interface, satellite_optimizer_interface],\n",
        "    [\"Rocket Launch\", \"Engine Maintenance\", \"Satellite Tasks\"]\n",
        ")\n",
        "\n",
        "combined_dashboard.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssdQ_ZC2bWmu",
        "outputId": "9262bf68-87c0-425d-e31b-8715376d3934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a4ea3e511d04e0657b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a4ea3e511d04e0657b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IpkNuLjpbfcY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMMvjV28FNXF78PlqGYqczO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}